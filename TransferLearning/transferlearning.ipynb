{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8f928c6",
   "metadata": {},
   "source": [
    "## 0. Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd83336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu118 0.22.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__, torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39b777e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory is now: c:\\Users\\Philippa\\Documents\\GitHub\n"
     ]
    }
   ],
   "source": [
    "# 1) Move “up” until you hit the folder that contains pytorch/\n",
    "import os\n",
    "os.chdir(os.path.abspath(os.path.join(os.getcwd(), os.pardir, os.pardir)))\n",
    "print(\"Working directory is now:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b36714b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "import sys\n",
    "from pytorch.TransferLearning.python_scripts import data_setup, engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc848287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce5d01",
   "metadata": {},
   "source": [
    "## 1. Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba9b90db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not find data\\pizza_steak_sushi directory, creating one...\n",
      "Downloading data...\n",
      "Unzipping data...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# setup path to data folder\n",
    "data_path = Path(\"data/\")\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} already exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "        print(\"Downloading data...\")\n",
    "        f.write(request.content)\n",
    "        \n",
    "    # unzip the data\n",
    "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "    \n",
    "    # remove .zip file\n",
    "    os.remove(data_path / \"pizza_steak_sushi.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99d8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up training and testing directories\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dbbf3",
   "metadata": {},
   "source": [
    "## 2. Create Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "233a3371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create transforms pipeline manually\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)), # reshape all images to 224x224, though some models may require different sizes\n",
    "    transforms.ToTensor(), # turn image values to between 0 and 1\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # a mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
    "                         std=[0.229, 0.224, 0.225]) # A standard deviation of [0.229, 0.224, 0.225] (across each colour channel)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01aaaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1df719d8980>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1df719c9590>,\n",
       " ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training and testing dataloaders and get a list of class names, using data_setup.py\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
    "                                                                               test_dir=test_dir,\n",
    "                                                                               transform=manual_transforms,\n",
    "                                                                               batch_size=32)\n",
    "\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17d18d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
